# PyNonC

# Introduction
We need a standardised way of performing analysis so that we can do the following:

- scalable high throughput analysis in an automated way, 
- track results and ensure they are searchable
- perform some kind of standardised error reporting

additionally, we have the following requirements: 

- store code must be stored in a centralised repository
- need to keep things simple enough so that everyone can contribute new code

A pipeline to perform analysis of non-coding genome sequence. 

The pipeline is based on the `Java` SmallRNAPipeline, but modified to try and make it simpler to use and customise (i.e. add new code).

# General Points

## Setting up the Python environment
Not being an expert in `Python` I'm not sure I've set things up the most effective way in order to access different packages within my environment. I am developing code using `Eclipse`, which seems to take care of paths, but when running the code from the command line, I first need to set the environment path 

```
export PYTHONPATH=$PYTHONPATH:'/home/simon/eclipse-workspace/pynonc/'
python ~/eclipse-workspace/pynonc/noncpipe/ncPipe.py -p /media/MyBookDuo/data/reactome/ncgenes_girona.json
```
Does anyone have a better suggestion about how to do this?

## Using a Python IDE
It seems that everyone is using a different IDE, but by comment consent, we agreed that everyone has [Spyder](https://www.spyder-ide.org/) installed. This will make it easier to work together on code.

## Thinking about code efficiency
We are working with large datasets - for example, we will ultimately be processing ~19 000 genes against 100 000 genomes. Thus, the effects of any inefficient code will be magnified as we scale up the analysis

we will come back to this with an example involving Panda Dataframes.

# Data
## General Points
1. Due to storage and data transfer limitations we are not working with the raw genome data. Instead, as we are interested in population variation, we can use the VCF files that have already been generated - this is also a sensible option as we don't know that much about variant calling. So far, we have only been working with the [`1000 Genomes Project`](https://www.internationalgenome.org/), so we don't know how standardised the data formats are beyond the VCF files. In particular, the information about super/sub- populations may be in different formats among different projects. 

2. To process and interpret the sequences, we need to use additional resources. So far, this includes Pathway databases (so far `Reactome.org`), annotation information (so far `ensembl.org`) and Gene reference resources to map `Reactome` data to `Ensembl` data (so far `UniProt`)

## 1000 Genomes

### VCF files
we are using liftover data generated by the Wellcome Trust (see [here](https://wellcomeopenresearch.org/articles/4-50) for publication, and [here](ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/supporting/GRCh38_positions) for the VCF files.

These are available as a single genome file, or as individual chromosomes, but the important difference between the genome file and the individual chromosomes is that the genome file only lists the SNVs for the five super populations (`EAS_AF`, `EUR_AF`, `AFR_AF`, `AMR_AF` and `SAS_AF`). to determine the presence of a particular SNV in an individual, we need to access the chromosome files. They probably did it this way to keep the file sizes manageable (<= 1GB)

Moreover, the individual chromosome files contain SNV phase information for each sample

However, the sample is only defined by a sample name and in order to get the population information, it is necessary to cross reference this with the [sample information file](https://www.internationalgenome.org/data-portal/sample) (look for the `Download the list` button)

### Samples file
The 1000 Genomes sample file has the following format:

|col|Field|
| ----------- | ----------- |
|1|Sample name|
|2|Sex|
|3|Biosample ID|
|4|Population code|
|5|Population name|
|6|Superpopulation code|
|7|Superpopulation name|
|8|Population elastic ID|
|9|Data collections|

I don't know if this is a standard format

The `PhyDist` software requires a summary of the super-/sub- populations. This can be generated from the intersection of the population information `igsr_samples` file and the samples included in the VCF file. However, there are 2548 samples in the VCF file, but 4973 samples in the `igsr_samples` file, so a little manipulation is required. This population information is stored in the following format:

|col|Field|
| --------- | --------- |
|0| ClassificationType|	
|1| ClassificationName|
|2| ClassificationDescription|

for example

|ClassificationType|ClassificationName|ClassificationDescription|
| ----- | ------ | ------ |
|SUB|FIN|Finnish in Finland|
|SUB|GBR|British in England and Scotland|
|SUB|IBS|Iberian Population in Spain|
|SUB|CEU|Utah Residents (CEPH) with Northern and Western European Ancestry|
|SUB|TSI|Toscani in Italia|
|SUB|CHS|Southern Han Chinese|
|SUB|CDX|Chinese Dai in Xishuangbanna, China|
|SUB|KHV|Kinh in Ho Chi Minh City, Vietnam|
|.|.|.|
|.|.|.|
|.|.|.|
|SUPER|EUR|European|
|SUPER|EAS|East Asians|
|SUPER|AMR|Ad Mixed Americans|
|SUPER|SAS|South Asian|
|SUPER|AFR|African|

generating this population should probably be implemented as a separate step


## Reactome.org
this is a key resource source for pathway information. From this, we can get lists of genes that are involved in a specific pathway. 

In the current release there are 2478 pathways and these have been downloaded from the [reactome.org](ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/idmapping/) ftp site as a Zip file in the `homo_sapiens.3.1.sbml/ folder`
The pathway information is stored in the System Biology Markup Language, with each pathway saved as a separate SBML file.

For what we need at the moment, which is to extract a list of genes in each pathway, almost all the information is redundant. We can get a gene list for a particular pathway by `grep`-ing for the string `uniprot`. For example, the following grep command will retrieve a list of all genes in the `Glucose metabolism` pathway

```
grep  uniprot /Users/simonray/Downloads/homo_sapiens.3.1.sbml/R-HSA-70326.sbml |sort|uniq > R-HSA-70326.genes 
```
```
$head R-HSA-70326.genes 
  <rdf:li rdf:resource="http://purl.uniprot.org/uniprot/A6NDG6" /> 
  <rdf:li rdf:resource="http://purl.uniprot.org/uniprot/A8CG34" /> 
  <rdf:li rdf:resource="http://purl.uniprot.org/uniprot/O00757" /> 
```
The Reactome SBML files specify gene information in terms of `UniProt` IDs, which leads to the next problem...

## UniProt
UniProt (uniprot.org) provides a comprehensive, high-quality resource of protein sequence and functional information. However, it doesn't provide gene location information in a readily accessible way. To do this, we need to get the corresponding Ensembl (ensembl.org) ids. Fortunately, UniProt provides mapping files with this information.

### mapping files
There are two formats, `.dat` and `.tsv` which have the following formats

####`idmapping.dat`
This file has three columns, delimited by tab:

1. UniProtKB-AC 
2. ID_type 
3. ID

where ID_type is the database name as appearing in UniProtKB cross-references, 
and as supported by the ID mapping tool on the UniProt web site, 
http://www.uniprot.org/mapping and where ID is the identifier in 
that cross-referenced database.

####`idmapping_selected.tab`
The tab file has more comprehensive information for each UniProt entry, and is what we used. Each tab delimited entry contains the following columns:


1. UniProtKB-AC
2. UniProtKB-ID
3. GeneID (EntrezGene)
4. RefSeq
5. GI
6. PDB
7. GO
8. UniRef100
9. UniRef90
10. UniRef50
11. UniParc
12. PIR
13. NCBI-taxon
14. MIM
15. UniGene
16. PubMed
17. EMBL
18. EMBL-CDS
19. Ensembl
20. Ensembl_TRS
21. Ensembl_PRO
22. Additional PubMed

Using the tsv format will allow us to extract additional annotation (such as Gene Ontology mappings) later on.

This file is stored as `~/Dropbox/data/reactome/HUMAN_9606_idmapping_selected.tab` and there are 194237 lines.

For now, extract the `UniProtKB-ID` (col 2), `geneID` (col 3) and `EnsemblIDs` (cols 19-21)`

### Problem 1: Multiple Mappings
One problem is that there are multiple entries for several genes
```
$cut -f 2-3 /Users/simonray/Dropbox/data/reactome/HUMAN_9606_idmapping_selected.tab|cut -f 2 |sort|uniq|wc -l
18986
```
or, in more detail

```
$cut -f 2-3 /Users/simonray/Dropbox/data/reactome/HUMAN_9606_idmapping_selected.tab|cut -f 2 |sort|uniq -c|sort -r|more
  31 3111
  29 7404
  25 3084
  17 4582
  15 3778
  15 274

```
However (I think) we can get around this as a gene can have multiple transcripts, but each ENST entry in the mapping file is unique, and from the Ensembl annotation, we can get the 3'UTR entry associated with that transcript

e.g.

```
1       havana  three_prime_UTR 70009 71585 .+. Parent=transcript:ENST00000641515
1       ensembl three_prime_UTR 70009 70108 .+. Parent=transcript:ENST00000335137
```

### Problem 2: Versioned UniProt IDs
for some reason, for certain genes, `Reactome` uses versioned `UniProt` IDs
For example, for `Q9Y6P5` the following names are found across all pathways

```
   6 Q9Y6P5
  15 Q9Y6P5-1
  15 Q9Y6P5-3
``` 
These point to different isoforms of the gene, but there is no corresponding entry in the UniProt map file. For example [`Q9Y6P5`](https://www.uniprot.org/uniprot/Q9Y6P5#Q9Y6P5-3)

this lists the following transcript variants in Ensembl

```
ENST00000302071﻿; ENSP00000306734﻿; ENSG00000080546 [Q9Y6P5-3]
ENST00000356644﻿; ENSP00000349061﻿; ENSG00000080546 [Q9Y6P5-1]
ENST00000436639﻿; ENSP00000393762﻿; ENSG00000080546 [Q9Y6P5-2]
```

But, if we look at the UniProt entry in the map file for `Q9Y6P5`, we only get the following transcripts
```
ENST00000302071; ENST00000356644; ENST00000436639
```
which is the first transcript for each entry

However, there is no entries for these versioned entries

```
$ grep 'Q9Y6P5-3' ../uniprot/HUMAN_9606_idmapping_selected.tab |wc -l
       0
$
```

so, for these aliases, we need to remove the version number to get a match in the map file

### Problem 3: Multiple ENSGs for one UniProt Entry
For example, `Q9NQR9` has two Ensembl gene mappings 
```
ENSG00000152254; ENSG00000278373
```
with the following transcript IDs
```
ENST00000282075; ENST00000375363; ENST00000429379; ENST00000612807; ENST00000617403; ENST00000622133
```
The UniProt entry for [`Q9NQR9`](https://www.uniprot.org/uniprot/Q9NQR9) lists both `ENSG` entries and appears to assign the first three transcripts to `ENSG00000152254` and the last three to `ENSG00000278373`

```
ENST00000282075﻿; ENSP00000282075﻿; ENSG00000152254 [Q9NQR9-2]
ENST00000375363﻿; ENSP00000364512﻿; ENSG00000152254 [Q9NQR9-1]
ENST00000429379﻿; ENSP00000396939﻿; ENSG00000152254 [Q9NQR9-3]

ENST00000612807﻿; ENSP00000481098﻿; ENSG00000278373 [Q9NQR9-3]
ENST00000617403﻿; ENSP00000483899﻿; ENSG00000278373 [Q9NQR9-2]
ENST00000622133﻿; ENSP00000482583﻿; ENSG00000278373 [Q9NQR9-1]
```

However, the Ensembl entries for [`ENSG00000152254`](https://www.ensembl.org/Homo_sapiens/Gene/Summary?g=ENSG00000152254;r=2:168901291-168910000) list the following transcripts

```
ENST00000612807.1	645		Protein coding	-	Q9NQR9-3	-	TSL:2GENCODE basic
ENST00000617403.1	2913	Nonsense mediated decay	-	Q9NQR9-2	-	TSL:1
ENST00000622133.4	1214	Protein coding	-	Q9NQR9-1	-	TSL:1GENCODE basic

ENST00000618310.1	494		Protein coding	-	C9IYU7	-	TSL:3GENCODE basic
ENST00000622908.1	1284	No protein	Processed transcript	-	-	-	TSL:1
```

and for [`ENSG00000278373`](https://www.ensembl.org/Homo_sapiens/Gene/Summary?g=ENSG00000152254;r=2:168901291-168910000)

```
ENST00000282075.5	2918	Nonsense mediated decay	-	Q9NQR9-2	-	TSL:1
ENST00000375363.8	3030	Protein coding	CCDS2230	Q9NQR9-1	NM_021176.3	TSL:1GENCODE basicAPPRIS P1MANE Select v0.91
ENST00000429379.2	645		Protein coding	CCDS46443	Q9NQR9-3	-	TSL:2GENCODE basic

ENST00000421979.1	494		Protein coding	-	C9IYU7	-	TSL:3GENCODE basic
ENST00000461586.1	1284	No protein	Processed transcript	-	-	-	TSL:1
```

The difference in the transcript lists is due to the `TSL` values. UniProt only retains the transcripts where `TSL = {1,2}`

However, one final oddity is that `ENSG00000278373` isn't present in the `gff3` annotation we are using here

```
grep 'ENSG00000278373' /Users/simonray/Dropbox/data/ensembl/Homo_sapiens.GRCh38.102.chr.gff3|wc -l
       0
```

### Problem 4: Missing ENSG entries in the GFF3 reference
entry `O43826` failed to return any ENSEMBL coordinates

There is is an ENSG entry in the map file `ENSG00000281500` but if 
we do the following, we find it is absent from the GFF3 annotation file

```
grep 'ENSG00000281500' /Users/simonray/Dropbox/data/ensembl/Homo_sapiens.GRCh38.102.chr.gff3|wc -l
       0
```

We can understand why by looking at the ENSEMBL entry for [´ENSG00000281500`](https://www.ensembl.org/Homo_sapiens/Gene/Summary?g=ENSG00000281500;r=CHR_HG2217_PATCH:119024112-119030905). 

The reference sequence is `Chromosome CHR_HG2217_PATCH: 119,024,112-119,030,905 reverse strand`. And we are missing this [`PATCH`](https://www.ensembl.org/Homo_sapiens/Location/View?db=core;g=ENSG00000281500;r=CHR_HG2217_PATCH:119024112-119030905) from our annotation.



### Task 1. Generate single mapping file for Reactome/ENSEMBL/Positions
Need the following steps
remove any entries that don't have Ensembl `ENST` information

```
$ grep 'ENST' HUMAN_9606_idmapping_selected.tab > HUMAN_9606_idmapping_selected_ENST.tab

$ wc -l HUMAN_9606_idmapping_selected.tab
  194237 HUselected_ENST.tab
   74774 HUMAN_9606_idmapping_selected_ENST.tab
``` 

and if we look at the Ensembl file

```
$grep -c three /Users/simonray/Downloads/Homo_sapiens.GRCh38.102.chr.gff3
167698
```
so at least there are more `three_prime_UTR` entries than `uniprot` transcription tags

Need to merge this two resources based on the `ENST` tags

#### getting SNVs that intersect with a specified feature

```
/usr/local/bin/bcftools view -H -r 10:69401166-69401168 /Users/simonray/Dropbox/data/1KGenomes/biallelic_SNVs_and_INDELs_GRCh38/ALL.chr10.shapeit2_integrated_snvindels_v2a_27022019.GRCh38.phased.vcf.gz > del.tsv
```


### Step 2. Generate annotated gene lists for all Reactome pathways
By annotated, this means integrating the additional data we will need for phylogenetic analysis.
The grep command above will only return `SwissProt` gene IDs. We need the `Ensembl` IDs to get location information and the standard gene name as this is what most people are familiar with. For example, `A6NDG6` is more commonly known as `PGP`.

I want to make everything as automated as possible, so the input data should be as follows:

a path to a folder containing the SBML files of interest
a 
This requires several steps. 
Input data:

a list of all pathways
a mapping of Swiss Prot to Ensembl


### Step 3. miRNA annotation

the GFF3 file has annotation for pre-miRNA entries. This is best as we can use the sequence for other analyses such as changes in MFE due to SNVs
```
grep miRNA /Users/simonray/Dropbox/data/ensembl/Homo_sapiens.GRCh38.102.chr.gff3|awk '{if ($3 == "miRNA") print $0;}'|awk -F';' '{print $3}'|awk -F'=' '{print $2}'|wc -l
    1879
```

this will generate a list of gene names that can be used to generate a gene list output similar to `Step 1`

the pre-mIRNA list is located in ``

#### miRNA relevance
How to measure a miRNA's importance?
Level of conservation across species is one option
For this, look at [`RFAM`](https://rfam.xfam.org/).
You can perform searches such as `miRNA AND entry_type:"Family"` that will returns a [list](https://rfam.xfam.org/search?q=miRNA%20AND%20entry_type:%22Family%22) of all families, including the number of species in which a specific miRNA was found. But there is no way to download these results.

instead, it is necessary to submit a `MySQL` query remotely to the RFAM database

A simple connectivity test can be performed using the following:

```
mysql --user rfamro --host mysql-rfam-public.ebi.ac.uk --port 4497 --database Rfam -e "show databases;"
```

and after some trial and error, this appears to give me the information I need

```
mysql --user rfamro --host mysql-rfam-public.ebi.ac.uk --port 4497 --database Rfam -e "SELECT fr.rfam_acc, fr.rfamseq_acc, fr.seq_start, fr.seq_end, f.type, f.description, tx.species FROM full_region fr, rfamseq rf, taxonomy tx, family f WHERE rf.ncbi_id = tx.ncbi_id AND f.rfam_acc = fr.rfam_acc AND fr.rfamseq_acc = rf.rfamseq_acc AND tx.tax_string LIKE '%Mammalia%' AND f.type LIKE '%miRNA%';" > /Users/simonray/Desktop/rfamd.csv
```

This is only selecting mammalian species, so should really use

```
mysql --user rfamro --host mysql-rfam-public.ebi.ac.uk --port 4497 --database Rfam -e "SELECT fr.rfam_acc, fr.rfamseq_acc, fr.seq_start, fr.seq_end, f.type, f.description, tx.species FROM full_region fr, rfamseq rf, taxonomy tx, family f WHERE rf.ncbi_id = tx.ncbi_id AND f.rfam_acc = fr.rfam_acc AND fr.rfamseq_acc = rf.rfamseq_acc AND f.type LIKE '%miRNA%';" > /Users/simonray/Desktop/rfam_mirna_allspecies.csv
```
# The Pipeline
Currently, the pipeline has three primary modules:  `noncpipe`, `coreClasses` and `pynoncsteps`.
 
I am using Python classes to define specific data objects. The main classes for running the project are located in the `noncpipe`, individual steps within the `pynoncsteps` module and useful classes (for example, for common tasks or to aid the readability of code) are located in `coreClasses`.

## The JSON project file
All project settings are specified using a JSON format. The format is

```
Project Definitions:
Step Definitions:
	[
	{Step1:},
	{Step2:},
		.
	{StepN:},
	]
```
### Project Definitions
These are specified one per line. For example:

```
 "pipelineName":"ncpipe", 
 "projectID":"1000KAllMB",
 "projectRoot":"/Users/simonray/Dropbox/dropData/",
 "dataRoot":"/data/",
```

### Step Definitions
These are specified as an array of JSON entries. For example:

```
[
    { "step": "NCStepGenConsensusSeqForMultiFeatures", 
      "parameters": " --sam_software_path=/usr/local/bin/samtools, --bcf_software_path=/usr/local/bin/bcftools",
      "inFolder": "geneLists", 
      "inFiles": ["R-HSA-165159_RP6_genelist.csv" ], 
      "outfolder": "geneLists"},
    { "step": "NCStepExit", "parameters": "", "inFolder": "", "inFiles": [], "outfolder": ""}, 
]
```


## The pipeline classes

### noncpipe:ncPipe.py
this is the top level class that is called to execute the pipeline. It handles all the tasks associated with setting up the pipeline.
It performs the following tasks:

1. create an instance of the `ncProject` class
2. generate a unique ID tag for the run, so that all generated output can be associated with the run
3. parse the run arguments
4. initiate logging
5. instruct the instance of `ncProject` to load the project file
6. instruct the instance of `ncProject` to parse the project file 
7. instruct the instance of `ncProject` to execute the project steps

#### Generate unique ID tag for the run
executing a multi step analysis will generate multiple files and we need to be able associate all these files with a specific run. Thus, at the start of the run, we generate a `MD5` hashtag that will be be used throughout the run. 
```
    md5String = hashlib.md5(b"CBGAMGOUS").hexdigest()
```
You can read more about `MD5` [here](https://www.md5hashgenerator.com/)

#### parsing run arguments
There is only one argument needed to run the pipeline

```
python3 ncPipe.py -p postnum_macbook.json
```
and this is handled by the `parseArgs` method
```
    parseArgs(argv)
```
#### initiate logging
The benefit of using `logging` rather `print` statements is we can control what is printed when we run the program. For example, in the `def initLogger(md5string):` method in `ncPipe.py`, we have the following lines

```
def initLogger(md5string):

    logging.basicConfig(level=logging.INFO)
    logging.info("project log file is <" + logfileName + ">") 
    logging.debug("debug mode is on")
```

this sets the `logging` level to `INFO`, so that any `logging.debug()` won't be printed. However, if we set the first line to 

```
def initLogger(md5string):

    logging.basicConfig(level=logging.DEBUG)
    logging.info("project log file is <" + logfileName + ">") 
    logging.debug("debug mode is on")
```
both lines will be printed. This is because `INFO` > `DEBUG`.
Thus, when writing your step, you will set `level=logging.DEBUG`. When you are confident your code is executing correctly, you can set `level=logging.INFO` so that a limited set of information is printed. If someone reports an error running your code, you can `level=logging.DEBUG` to get more a more detailed report to help you trap the error.

Thus, there should *never* be any print statements in the code, you should always use `logging` statements. The only exception is in the `ncPipe.py` class during initialisation. We need the name of the JSON project file to generate the matching logfile name so we use a print statement to print out the project filename that was parsed out from the runtime arguments.
```
        print(projectFile)
```
#### Loading the Project File
This is in `JSON` format. The format recognised by the pipeline is fixed. *If you want to make a change, it has to be by common consent, you can't just change the format.* This is performed by the instance of `ncProject` and described in the next section. 

#### parsing the project file
This parses the variable defined in the JSON files and reads the project steps. This is performed by the instance of `ncProject` and described in the next section.

#### executing the project steps
This executes each step defined in the project file. This is performed by the instance of `ncProject` and described in the next section.

### noncpipe:ncProject.py
This class handles all operations that are associated with the project.

#### loading the project file
This uses the JSON package `json` to load the project file:

```
import json
	.
	.
	.

    def loadProjectFile(self):
        with open(self.projectFile) as f:
            projectJSON = json.load(f)
            

```
and maps the project definitions in the JSON file to the corresponding class variables
JSON definitions:

```
 "pipelineName":"ncpipe", 
 "projectID":"1000KAllMB",
 "projectRoot":"/Users/simonray/Dropbox/dropData/",
 "dataRoot":"/data/",
```

class variables:

```
	 self.projectID = projectJSON[noncConstants.NoncConstants.PROJECT_ID]
	 self.projectRoot = projectJSON[noncConstants.NoncConstants.PROJECT_ROOT]
	 self.pipelineName = projectJSON[noncConstants.NoncConstants.PIPELINE_NAME]
```

#### Building and executing the steps
Currently, this is carried out as a single process. As the steps are specified as a nested list, when they are read in they are stored as a `Python list` and we can just iterate through the list to process each step in order:

```
def loadProjectFile(self):
 	 self.projectSteps = projectJSON[noncConstants.NoncConstants.PROJECT_STEPS]
 	 
def buildStepSet(self): 	 
    for step in self.projectSteps:
    
```
Then, for each step, we do the following:

##### 1. get the step type
We use a `factory` class to create an instance of the step
This uses the step name specified specified in the json string for this step (e.g. `"step": "NCStepGrabSNVsForFeatures"`)

```
		thisStep = vStepFactory.get_step(step['step']) 
```    

##### 2. Copy the project information to the step

```
		thisStep.projectRoot = self.projectRoot
		thisStep.projectID = self.projectID
		thisStep.md5string = self.md5string
		thisStep.jsonString = step
```        

##### 3. Parsing the JSON string for the step

```     
		thisStep.parseJSON(step)
```        
As each entry is a JSON string, it needs to be parsed. This is performed by the step and is described in the next section.


##### 4. parse the parameter string

```
		thisStep.parseParameterString()
```
This parses the parameter string for the step. This is performed by the step and is described in the next section.

The parameter string contains information that is specific to the step. For example, every step has an `inputFolder` and `outputFolder`, but a step may need the path to a particular software package or reference data resource. Each parameter is delimited by a "," and has the format `key=value`. For example:

``` 
"parameters": "--sam_software_path=/usr/local/bin/samtools, 
--bcf_software_path=/usr/local/bin/bcftools",
```

##### 5. check the input data

```            
		thisStep.checkInputData()
```
This checks that the input data specified by the `inputFolder`, `outputFolder` and `parameterString` variables are accessible. This is performed by the step and is described in the next section.

##### 6. Execute the step

```                
		thisStep.execute()
```
This method is where the main execution of the step occurs. This is performed by the step and is described in the next section.


### Exception Handling
It's better to try and catch errors rather than letting the program exit unexpectedly. You can't everything, but there are some things you can look out. For example, in the `NCStepReadPositiveNumber` step, we are parsing a parameter to read a number, but there is a possibility the user specified a string, so we can use a `try .. catch` statement:

```
        try:    
            if self.positiveNumber < 0:
                logging.error("--positiveNumber <" + self.positiveNumber + "> is less than zero")
                raise RuntimeError ("--positiveNumber <" + self.positiveNumber + "> is less than zero")
            else:
                logging.info(INDENT*'-' + "--positiveNumber <" + self.positiveNumber + "> passed less than zero check")
            
        except ValueError:
            logging.error("--positiveNumber <" + self.positiveNumber + "> is not a number")
            raise RuntimeError ("--positiveNumber <" + self.positiveNumber + "> not a number")            
            return 0

```

## 

### pynoncsteps:abstractNCStep
All the steps are 'inherited' from the abstract `abstractNCStep` class. An abstract class cannot be executed, but if we derive a class from an Abstract class, that class has to follow the rules that are specifed in that class. The class is declared to be abstract by including the following line at the top of the class

```
from abc import ABC, abstractmethod
```

then, in the class that inherits `abstractNCStep`, we specify the following to inform it is derived from an Abstract class:

```
class NCStepReadPositiveNumber(abstractNCStep.AbstractNCStep):
```

Then, in `abstractNCStep` we specify the following:

#### 1. Class Constants

```
    PARAMID         = "parameters"
    INFOLDERID      = "inFolder"
    INFILESID       = "inFiles"
    OUTFOLDERID     = "outFolder"
```

#### 2. Class Variables
```
    self.projectFile = ""
    self.projectID = ""
    self.projectRoot = ""
    self.jsonString = ""
    self.inFolder = ""
    self.inputFiles = []
    self.outFolder = ""
    self.outputFiles = []
    self.paramString = ""        
    self.md5string = ""
```

#### 3. Abstract Methods

```
    @abstractmethod
    def shortDescription(self):
        pass
    
    @abstractmethod
    def longDescription(self):
        pass
    
    @abstractmethod
    def parseParameterString(self):
        pass

    @abstractmethod
    def checkInputData(self):
        pass    
        
    @abstractmethod
    def execute(self):
        pass      
```

Additionally, the `abstractNCStep` contains an additional method:

```        
    def parseJSON(self, stepJSON):
```
`parseJSON ` is not defined as an `abstractmethod` since every step has the same format. Thus, the parsing is identical and we can define this method once for all classes.
 
### pynoncsteps:NCStepReadPositiveNumber
A functioning step needs to implement each of the abstractmethods to be allowed to run - if they aren't Python will throw an error. 

To demonstrate this, the project includes a simple implementation of a step, `NCStepReadPositiveNumber` that reads a positive number defined in the `"parameters":` field of the step JSON and writes it to a file specified in the `"outfolder":` field.

#### 1. def shortDescription(self):
This provides a user with a one line description of what a step does. Using this, a user can quickly scan all steps to see if a Step already exists that does what he/she wants.

#### 2. longDescription(self):
This provides detailed information about what a step does, what information is needed for input (including what types of data files), what parameters are needed to provide the step with the data it needs to complete its analysis, and what type of output is generated. Ideally, it should also provide a sample of the JSON string needed to run the step

#### 3. parseParameterString(self):
This parses out the `parameterString` with the JSON string for the step. This will copy the values into the step variables you define in `def __init__(self):`. 

#### 4. def checkInputData(self)
This checks the data before the step is run. The idea behind this is that, if you are running an analysis that could take several days or hours to run, it is better to check you have access all the data for every step *before* you start the run. This reduces the chance of the pipeline throwing an error in the middle of the run - particularly annoying if you are running over the weekend and find the pipeline failed 7pm Friday.

Typically, you should check that

* the `inputFolder` exists
* any data folders, data files, or paths to software defined in the `parameterString` exist in the defined locations
* parameters have reasonable values (for example, in `NCStepReadPositiveNumber`, the step reads a positive number, so we should check the 1. the value is a number, 2. the value is > 0.


#### 5. def execute(self):
This is where the code to execute the main goal of the step is placed. If the method starts getting long, you can always break subtasks into methods that are called within the `execute` method.


### Core classes
There are a number of core classes that have been defined to handle common tasks and to aid readability. For example, working with [`GFF3` format](https://learn.gencore.bio.nyu.edu/ngs-file-formats/gff3-format/) is a common task and creating/parsing `GFF3` lines is a piece of code that will be reused. Therefore, creating a class to handle this seems reasonable. The downside is that it requires more overhead to create a class instance, but this is balanced by the fact that it is simpler to read/write `GFF3` lines in your code. 

For example, the following code gets the location information from a `GFF3` entry:

```
from coreClasses import gffLine

    chromosome = gffLine.GFFLine.getChrom(gLine)
    start = gffLine.GFFLine.getStart(gLine)
    end = gffLine.GFFLine.getEnd(gLine)
    strand = gffLine.GFFLine.getStrand(gLine)
```
This is more readable than 

```
noncpipe.noncConstants.py:
    ENS_CHR_COL             =   0
    ENS_SRC_COL             =   1
    ENS_FEAT_COL            =   2
    ENS_START_COL           =   3
    ENS_END_COL             =   4
    ENS_SCORE_COL           =   5
    ENS_STRAND_COL          =   6
    ENS_PHASE_COL           =   7
    END_ATTRIBUTES_COL      =   8


from noncpipe import noncConstants

    chromosome = tLine.split("\t")[noncConstants.NoncConstants.ENS_CHR_COL]
    source = tLine.split("\t")[noncConstants.NoncConstants.ENS_SRC_COL]
    start = int(tLine.split("\t")[noncConstants.NoncConstants.ENS_START_COL])
    end = int(tLine.split("\t")[noncConstants.NoncConstants.ENS_END_COL])
    strand = tLine.split("\t")[noncConstants.NoncConstants.ENS_STRAND_COL]
```
The other advantage is that the `gffLine.GFFLine` method has already been tested. In the second case, you have to debug the code everytime you want to parse a `GFF3` line.

### Other things to do when adding a new step


#### 1. registering the class with the ClassFactory

```
from pynoncsteps.ncStepReadPositiveNumber import NCStepReadPositiveNumber

    def get_step(self, classID):
        if classID == NCStepReadPositiveNumber.CLASSID:
            return NCStepReadPositiveNumber()
```

#### 2. parameters
I try to use `String` constants when possible, rather than hard coding. For example:

```
    POSNUMBERSHORT      = "-p"                                      #
    POSNUMBERLONG       = "--positive_number"                       #
    
    if key == self.POSNUMBERSHORT or key == self.POSNUMBERLONG:
        if key == self.POSNUMBERLONG:
            self.positiveNumber = value
        else:
            self.positiveNumber = value
        logging.info(INDENT*'-' + "positive number set to <" + str(self.positiveNumber) + ">")      
```

This is for two reasons. 

(1) it makes it easier to change a parameter.
For example, if i want to change the parameter to `-p/--posnum`, i just need to change `POSNUMBERLONG = "--posnum"`
(2) it makes it easier porting the code to add a new parameter. For example, to add a second parameter, `-n/--negative_num`, I just need to add two new string constants:

```
    NEGNUMBERSHORT      = "-n"                                      #
    NEGNUMBERLONG       = "--negative_number"                       #
```

copy the `if` statement and do a global replace for (i) `POSNUMBER` --> `NEGNUMBER` and (ii) `positive` --> `negative`

```
    NEGNUMBERSHORT      = "-n"                                      #
    NEGNUMBERLONG       = "--negative_number"                       #
    
    if key == self.NEGNUMBERSHORT or key == self.NEGNUMBERLONG:
        if key ==self.NEGNUMBERLONG:
            self.negativeNumber = value
        else:
            self.negativeNumber = value
        logging.info(INDENT*'-' + "negative number set to <" + str(self.negativeNumber) + ">")      
```




## Creating a new step
the easiest way is to copy an existing step
I am using `Eclipse` so I just use `copy`/`paste` a class located in the `pynoncsteps` package. I try to use a class that isn't too big as i find it easier to modify

1. rename the file
2. rename the class inside the file
3. rename the `CLASSID` constant
4. update the `shortDescription` and `longDescription` methods
5. add reference to the `ncStepFactory` class 
```
from pynoncsteps.ncStepGenConsensusSeqForFeatures import NCStepGenConsensusSeqForFeatures
```
6. add case to the `get_step` method
7. update the `checkInputData(self)` method
8. update the `def __init__()` method to define any `self` variables
9. add constants for parameters to avoid hard coding in the code
```
    BCFSOFTWARELOCLONG  = "--bcf_software_path"                     #
    BCFSOFTWARELOCSHORT = "-b"                                      #
```
10. update `def parseParameterString(self):` method


start with `buildStepSet(self):` for debugging/testing